{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T03:09:39.991773Z",
     "start_time": "2018-07-19T03:09:39.760910Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "\n",
    "raw = pd.read_table('/Users/minhmai/Downloads/train.txt', delimiter=' ', header=None)\n",
    "raw.columns = ['word', 'pos', 'wsj']\n",
    "\n",
    "\n",
    "def refactor_pos(x):\n",
    "    # helps deal with imbalances between classes\n",
    "    if x == '.':\n",
    "        return 'stopper'\n",
    "    elif x.startswith('V') or x.startswith('RB') or x == 'JJ':\n",
    "        return 'verb/adverb/adjective'\n",
    "    elif x in ['NS', 'NNS', 'NN']:\n",
    "        return 'noun'\n",
    "    elif x in ['IN', 'TO', 'DT']:\n",
    "        return 'preposition'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def preprocess_dataframe(data):\n",
    "    regex = re.compile(r'[^.a-z0-9]')\n",
    "    data['is_symbol'] = data.pos.apply(lambda x: True if regex.match(x) else False)\n",
    "    data = data[((data.is_symbol == True) | (data.pos == '.'))]\n",
    "\n",
    "    # refactor pos tagging\n",
    "    data['pos'] = data.pos.apply(lambda x: refactor_pos(x))\n",
    "    data = data[['word', 'pos']]\n",
    "    data['word'] = data.word.apply(lambda x: x.lower())\n",
    "    corpus = {k: v for k, v in zip(data['word'].unique(), range(data['word'].nunique()))}\n",
    "    pos_corpus = {k: v for k, v in zip(data['pos'].unique(), range(1, data['pos'].nunique())) if v != 'stopper'}\n",
    "    pos_corpus['stopper'] = 9\n",
    "    data['word'] = data.word.apply(lambda x: corpus[x])\n",
    "    data['pos'] = data.pos.apply(lambda x: pos_corpus[x])\n",
    "    return corpus, pos_corpus, data\n",
    "\n",
    "def determine_max_length(data):\n",
    "    array = [0] + idx + [data.shape[0]]\n",
    "    start = 0\n",
    "    max_num = 0\n",
    "    for i in array:\n",
    "        val = i - start\n",
    "        if val > max_num:\n",
    "            max_num = val\n",
    "        start = i\n",
    "    return max_num\n",
    "\n",
    "def create_sentence_vectors(data):\n",
    "    idx = data.loc[data['pos'] == 9, :].index.values.tolist()\n",
    "    words = np.empty([122, ])\n",
    "    pos = np.empty([122, ])\n",
    "    start = 0\n",
    "    max_length = determine_max_length(data)\n",
    "    for i in idx:\n",
    "        _words = data.loc[start:i, 'word'].values\n",
    "        _pos = data.loc[start:i, 'pos'].values\n",
    "        start = i + 1\n",
    "        words = np.vstack((words, np.hstack((_words, np.zeros(max_length - len(_words))))))\n",
    "        pos = np.vstack((pos, np.hstack((_pos, np.zeros(max_length - len(_pos))))))\n",
    "    return words, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T02:55:56.622473Z",
     "start_time": "2018-07-19T02:55:56.050031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 2., 1., 3., 3., 3., 2., 3., 2., 3., 1., 2., 1., 1., 2., 4.,\n",
       "       4., 3., 2., 1., 1., 4., 3., 2., 3., 2., 3., 1., 2., 4., 4., 4., 4.,\n",
       "       3., 1., 9., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, pos_corpus, data = preprocess_dataframe(raw)\n",
    "words, pos = create_sentence_vectors(data)\n",
    "# pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T03:12:11.865895Z",
     "start_time": "2018-07-19T03:12:11.457374Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[  0.,   1.,   1., ...,   0.,   0.,   0.],\n       [  0.,   1.,   2., ...,   0.,   0.,   0.],\n       [ 34.,  35.,   2., ...,   0.,   0.,   0.],\n       ...,\n       [699., 885., 886., ...,   0., ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-12511cf9550f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m~/Envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/py3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[  0.,   1.,   1., ...,   0.,   0.,   0.],\n       [  0.,   1.,   2., ...,   0.,   0.,   0.],\n       [ 34.,  35.,   2., ...,   0.,   0.,   0.],\n       ...,\n       [699., 885., 886., ...,   0., ..."
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 5  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "num_tokens = determine_max_length(data)\n",
    "max_length = determine_max_length(data)\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_tokens))\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_tokens))\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True)\n",
    "decoder_outputs = decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "decoder_dense = Dense(num_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(\n",
    "    words,\n",
    "    pos,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T03:25:47.892557Z",
     "start_time": "2018-07-19T03:25:47.886322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52., 53., 54., 55., 56., 15., 48., 44., 57., 58., 59.,  2., 34.,\n",
       "       30., 60.,  7., 61., 62., 63., 43., 64.,  1., 65., 66., 67., 68.,\n",
       "       69., 70., 33.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-19T03:25:50.311971Z",
     "start_time": "2018-07-19T03:25:50.306097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 1., 3., 3., 1., 2., 1., 3., 3., 3., 2., 2., 1., 4., 1., 2., 3.,\n",
       "       2., 3., 1., 1., 2., 4., 4., 4., 1., 3., 4., 9., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
